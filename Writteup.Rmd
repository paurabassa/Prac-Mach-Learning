---
title: "Practical Machine Learning Assigment"
author: "Pau Rabassa"
date: "20/06/2015"
output: html_document
---

## Sumary
This is an R Markdown document to perform an analysis 
of the *Human Activity Recognition* data available 
at <http://groupware.les.inf.puc-rio.br/har>. 
The main objective of the analysis is to predict the category of 
5 different excercices classes based on different sensors data. 

## Load and prepare the data

To prepare the data I define the `count.na.feat` auxiliary function that counts the number 
of "NA" variables in each colum of a dataset. 
```{r, cache=TRUE}
count.na.feat <- function(dt.frm){ 
   sapply(1:ncol(dt.frm), function(i) { sum(is.na(dt.frm[,i]))})
}
```

Then I read the test file and remove all variables that are not defined. I also exclude some
variables like the index and the timestamp because they are no meaninful from a physical 
point of view. 

```{r, cache=TRUE}
setwd("~/DataScienceCoursera/Prac-Machine-Learning/Project/")
tst.raw <- read.csv("pml-testing.csv")
noselect <- which(count.na.feat(tst.raw) > 0 )
noselect <- c(1,3:5,noselect)
tst <- tst.raw[,-noselect]
sum(count.na.feat(tst)>0)
```

I read the train file, remove the same variables and check that there are no NA left
```{r, cache=TRUE}
trn.raw <- read.csv("pml-training.csv")
trn <- trn.raw[,-noselect]
sum(count.na.feat(trn)>0)
```

Once the data is clean, I split the original train set into a training and a testing set in 
order to asses the out of sample error later on. 
```{r, cache=TRUE, results='hide'}
library(caret)
outcomes <- trn[,ncol(trn)]
id.train <- createDataPartition(outcomes, p = 0.60,list=FALSE)
training <- trn[ id.train, -ncol(trn)]
testing  <- trn[-id.train, -ncol(trn)]
```

## Analysis

Let's take a look at the classes of the different variables in the training set: 
```{r, cache=TRUE}
sapply(1:ncol(training),function(i){class(training[,i])})
```
The training set shows a mixture of `factor`, `integer` and `numeric` variables. Random 
forest seem a suitable method since it implies little effort on preparing the data 
(no creation of dummy variable needed, etc...)

```{r, cache=TRUE}
# Random forest fit
trainingrf <- data.frame(outcomes = outcomes[id.train], training) 
fit.rf <- train(outcomes ~ ., data=trainingrf) #this line takes a while
fit.rf
```

The method show very high accuracy, but this could be due to overfitting. I can use now the 
testing set to get an idea of which is the out of sample error. 

```{r, cache=TRUE}
confusionMatrix(data=predict(fit.rf, newdata = testing),outcomes[-id.train])
#varImp(fit.rf)
```

The confusion matrix confirms that the high accuracy is for real 
and not due to overfitting. So we will expect over 99% of the activities in the 
test (the original one) set to be classified correctly. 

## Prepare Submission

Finally I compute the answers to the assigment
```{r, cache=TRUE}
answers = predict(fit.rf,tst)
answers
```

and write the answers into files for submission. 
```{r, cache=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
 
pml_write_files(answers)
```
